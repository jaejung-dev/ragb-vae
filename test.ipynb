{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, sys\n",
        "from pathlib import Path\n",
        "import yaml\n",
        "import torch\n",
        "from matplotlib import pyplot as plt\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "# Make project imports work (in notebooks __file__ is undefined)\n",
        "ROOT = Path.cwd()\n",
        "sys.path.append(str(ROOT))\n",
        "\n",
        "from src.training.rgba_vae_stage import build_dataloader  # noqa: E402\n",
        "\n",
        "# Load config\n",
        "cfg_path = ROOT / \"configs/flux_vae.yaml\"\n",
        "with cfg_path.open(\"r\", encoding=\"utf-8\") as f:\n",
        "    cfg = yaml.safe_load(f)\n",
        "\n",
        "# Force a small batch for quick visual checks\n",
        "cfg.setdefault(\"data\", {})\n",
        "cfg[\"data\"][\"batch_size\"] = 16\n",
        "cfg[\"data\"][\"shuffle\"] = True  # enable shuffle for probing\n",
        "cfg[\"data\"][\"interleave_buckets\"] = True  # mix buckets instead of draining one-by-one\n",
        "cfg[\"data\"][\"background_blend_prob\"] = 1.0\n",
        "# Build dataloader on CPU\n",
        "train_loader = build_dataloader(cfg, split=\"train\")\n",
        "print(\"Dataloader ready. Batches:\", len(train_loader) if hasattr(train_loader, \"__len__\") else \"unknown\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total entries: 2831275\n",
            "bucket  w1024-h1024: 2058520 samples (72.706%)\n",
            "bucket   w960-h1152: 286172 samples (10.108%)\n",
            "bucket   w832-h1024: 274737 samples (9.704%)\n",
            "bucket   w768-h1408: 61603 samples (2.176%)\n",
            "bucket   w832-h1280: 35790 samples (1.264%)\n",
            "bucket   w1024-h832: 18288 samples (0.646%)\n",
            "bucket   w1408-h768: 9933 samples (0.351%)\n",
            "bucket   w1280-h832: 5546 samples (0.196%)\n",
            "bucket    w768-h512: 4582 samples (0.162%)\n",
            "bucket   w1216-h896: 4181 samples (0.148%)\n",
            "bucket    w576-h576: 3924 samples (0.139%)\n",
            "bucket    w768-h576: 3517 samples (0.124%)\n",
            "bucket    w768-h768: 3182 samples (0.112%)\n",
            "bucket   w1024-h704: 2945 samples (0.104%)\n",
            "bucket   w1024-h640: 2731 samples (0.096%)\n",
            "bucket   w896-h1216: 2620 samples (0.093%)\n",
            "bucket   w1024-h768: 2445 samples (0.086%)\n",
            "bucket   w704-h1024: 2064 samples (0.073%)\n",
            "bucket    w576-h896: 2033 samples (0.072%)\n",
            "bucket    w576-h768: 2022 samples (0.071%)\n",
            "bucket    w640-h640: 2022 samples (0.071%)\n",
            "bucket    w960-h768: 1466 samples (0.052%)\n",
            "bucket   w768-h1024: 1389 samples (0.049%)\n",
            "bucket    w576-h832: 1369 samples (0.048%)\n",
            "bucket   w1216-h768: 1333 samples (0.047%)\n",
            "bucket    w896-h576: 1314 samples (0.046%)\n",
            "bucket    w960-h640: 1311 samples (0.046%)\n",
            "bucket   w768-h1088: 1211 samples (0.043%)\n",
            "bucket    w640-h960: 1135 samples (0.040%)\n",
            "bucket   w1280-h704: 1021 samples (0.036%)\n",
            "bucket   w1024-h576: 1007 samples (0.036%)\n",
            "bucket    w704-h512: 986 samples (0.035%)\n",
            "bucket    w704-h704: 924 samples (0.033%)\n",
            "bucket    w768-h960: 815 samples (0.029%)\n",
            "bucket    w576-h704: 772 samples (0.027%)\n",
            "bucket    w832-h576: 745 samples (0.026%)\n",
            "bucket   w768-h1152: 730 samples (0.026%)\n",
            "bucket    w512-h512: 726 samples (0.026%)\n",
            "bucket   w1280-h768: 724 samples (0.026%)\n",
            "bucket   w1344-h832: 683 samples (0.024%)\n",
            "bucket    w512-h768: 651 samples (0.023%)\n",
            "bucket    w640-h896: 632 samples (0.022%)\n",
            "bucket   w768-h1216: 609 samples (0.022%)\n",
            "bucket    w704-h896: 593 samples (0.021%)\n",
            "bucket   w640-h1024: 582 samples (0.021%)\n",
            "bucket   w1152-h768: 559 samples (0.020%)\n",
            "bucket    w896-h640: 550 samples (0.019%)\n",
            "bucket   w1216-h640: 531 samples (0.019%)\n",
            "bucket    w896-h704: 525 samples (0.019%)\n",
            "bucket   w704-h1088: 515 samples (0.018%)\n",
            "bucket    w640-h768: 512 samples (0.018%)\n",
            "bucket   w1088-h704: 507 samples (0.018%)\n",
            "bucket    w768-h640: 485 samples (0.017%)\n",
            "bucket    w832-h512: 484 samples (0.017%)\n",
            "bucket    w704-h960: 481 samples (0.017%)\n",
            "bucket    w640-h832: 480 samples (0.017%)\n",
            "bucket    w960-h704: 479 samples (0.017%)\n",
            "bucket    w576-h640: 442 samples (0.016%)\n",
            "bucket   w1216-h704: 428 samples (0.015%)\n",
            "bucket    w896-h896: 426 samples (0.015%)\n",
            "bucket   w1152-h960: 400 samples (0.014%)\n",
            "bucket    w512-h704: 386 samples (0.014%)\n",
            "bucket   w896-h1152: 385 samples (0.014%)\n",
            "bucket    w768-h896: 381 samples (0.013%)\n",
            "bucket    w960-h576: 374 samples (0.013%)\n",
            "bucket    w704-h576: 370 samples (0.013%)\n",
            "bucket    w640-h512: 345 samples (0.012%)\n",
            "bucket    w640-h576: 345 samples (0.012%)\n",
            "bucket   w1216-h832: 330 samples (0.012%)\n",
            "bucket    w576-h512: 305 samples (0.011%)\n",
            "bucket    w768-h704: 305 samples (0.011%)\n",
            "bucket    w512-h640: 299 samples (0.011%)\n",
            "bucket    w576-h384: 284 samples (0.010%)\n",
            "bucket    w960-h512: 278 samples (0.010%)\n",
            "bucket   w1408-h704: 273 samples (0.010%)\n",
            "bucket    w960-h960: 265 samples (0.009%)\n",
            "bucket    w704-h768: 263 samples (0.009%)\n",
            "bucket    w384-h896: 255 samples (0.009%)\n",
            "bucket   w1152-h896: 254 samples (0.009%)\n",
            "bucket    w832-h640: 245 samples (0.009%)\n",
            "bucket   w960-h1088: 232 samples (0.008%)\n",
            "bucket    w768-h832: 223 samples (0.008%)\n",
            "bucket  w1024-h1088: 210 samples (0.007%)\n",
            "bucket   w1344-h768: 196 samples (0.007%)\n",
            "bucket   w1088-h960: 191 samples (0.007%)\n",
            "bucket    w896-h512: 188 samples (0.007%)\n",
            "bucket    w704-h832: 184 samples (0.006%)\n",
            "bucket    w640-h704: 173 samples (0.006%)\n",
            "bucket   w1152-h640: 171 samples (0.006%)\n",
            "bucket   w832-h1344: 168 samples (0.006%)\n",
            "bucket  w1088-h1024: 165 samples (0.006%)\n",
            "bucket    w832-h832: 165 samples (0.006%)\n",
            "bucket   w576-h1024: 162 samples (0.006%)\n",
            "bucket   w1088-h768: 157 samples (0.006%)\n",
            "bucket    w704-h640: 157 samples (0.006%)\n",
            "bucket   w1088-h832: 153 samples (0.005%)\n",
            "bucket   w832-h1088: 153 samples (0.005%)\n",
            "bucket    w512-h576: 147 samples (0.005%)\n",
            "bucket   w1024-h960: 139 samples (0.005%)\n",
            "bucket   w1024-h512: 138 samples (0.005%)\n",
            "bucket    w576-h960: 134 samples (0.005%)\n",
            "bucket   w1088-h640: 132 samples (0.005%)\n",
            "bucket   w896-h1024: 114 samples (0.004%)\n",
            "bucket    w896-h768: 105 samples (0.004%)\n",
            "bucket    w512-h832: 103 samples (0.004%)\n",
            "bucket   w1088-h576: 103 samples (0.004%)\n",
            "bucket   w1024-h896: 102 samples (0.004%)\n",
            "bucket   w768-h1280: 100 samples (0.004%)\n",
            "bucket   w832-h1216: 95 samples (0.003%)\n",
            "bucket   w896-h1088: 94 samples (0.003%)\n",
            "bucket    w512-h384: 89 samples (0.003%)\n",
            "bucket   w960-h1024: 86 samples (0.003%)\n",
            "bucket   w768-h1344: 72 samples (0.003%)\n",
            "bucket   w1088-h896: 69 samples (0.002%)\n",
            "bucket    w832-h704: 62 samples (0.002%)\n",
            "bucket    w832-h768: 61 samples (0.002%)\n",
            "bucket   w704-h1408: 59 samples (0.002%)\n",
            "bucket   w1152-h704: 58 samples (0.002%)\n",
            "bucket   w640-h1152: 49 samples (0.002%)\n",
            "bucket    w832-h960: 49 samples (0.002%)\n",
            "bucket   w1152-h832: 48 samples (0.002%)\n",
            "bucket    w960-h832: 47 samples (0.002%)\n",
            "bucket   w832-h1152: 46 samples (0.002%)\n",
            "bucket    w896-h832: 46 samples (0.002%)\n",
            "bucket   w576-h1088: 43 samples (0.002%)\n",
            "bucket    w512-h960: 41 samples (0.001%)\n",
            "bucket   w704-h1216: 40 samples (0.001%)\n",
            "bucket    w832-h896: 39 samples (0.001%)\n",
            "bucket   w640-h1088: 38 samples (0.001%)\n",
            "bucket   w1280-h896: 36 samples (0.001%)\n",
            "bucket    w896-h960: 34 samples (0.001%)\n",
            "bucket   w704-h1152: 31 samples (0.001%)\n",
            "bucket   w704-h1280: 30 samples (0.001%)\n",
            "bucket    w960-h896: 28 samples (0.001%)\n",
            "bucket   w896-h1280: 26 samples (0.001%)\n",
            "bucket    w512-h896: 24 samples (0.001%)\n",
            "bucket   w1152-h576: 22 samples (0.001%)\n",
            "bucket   w1280-h640: 22 samples (0.001%)\n",
            "bucket   w512-h1024: 17 samples (0.001%)\n",
            "bucket   w640-h1216: 17 samples (0.001%)\n",
            "bucket    w384-h768: 15 samples (0.001%)\n",
            "bucket   w1216-h576: 11 samples (0.000%)\n",
            "bucket    w832-h448: 10 samples (0.000%)\n",
            "bucket   w1344-h704: 7 samples (0.000%)\n",
            "bucket   w576-h1152: 6 samples (0.000%)\n",
            "bucket   w640-h1280: 4 samples (0.000%)\n",
            "bucket   w704-h1344: 4 samples (0.000%)\n",
            "bucket   w512-h1088: 3 samples (0.000%)\n",
            "bucket   w576-h1216: 1 samples (0.000%)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- First 200 batches bucket frequency (with shuffle=True)\n",
            "bucket  w1024-h1024: 150 batches\n",
            "bucket   w960-h1152: 23 batches\n",
            "bucket   w832-h1024: 15 batches\n",
            "bucket   w832-h1280: 5 batches\n",
            "bucket   w1024-h832: 3 batches\n",
            "bucket   w768-h1408: 3 batches\n",
            "bucket    w960-h576: 1 batches\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "from src.data_generation.bucket_dataset import BucketBatchSampler\n",
        "\n",
        "# Bucket size stats\n",
        "bucket_counts = {k: len(v) for k, v in train_loader.dataset.bucket_to_indices.items()}\n",
        "total_entries = sum(bucket_counts.values())\n",
        "print(\"Total entries:\", total_entries)\n",
        "for b, c in sorted(bucket_counts.items(), key=lambda x: x[1], reverse=True):\n",
        "    print(f\"bucket {b:>12}: {c} samples ({c/total_entries:.3%})\")\n",
        "\n",
        "# Simulate sampler order without loading images\n",
        "num_batches_probe = 200\n",
        "sampler = BucketBatchSampler(\n",
        "    train_loader.dataset.bucket_to_indices,\n",
        "    batch_size=cfg[\"data\"][\"batch_size\"],\n",
        "    shuffle=True,\n",
        "    drop_last=bool(cfg[\"data\"].get(\"drop_last\", False)),\n",
        "    interleave=True,\n",
        ")\n",
        "probe = Counter()\n",
        "for i, batch_idxs in enumerate(sampler):\n",
        "    bname = train_loader.dataset.entries[batch_idxs[0]][\"bucket\"]\n",
        "    probe[bname] += 1\n",
        "    if i + 1 >= num_batches_probe:\n",
        "        break\n",
        "print(\"--- First\", num_batches_probe, \"batches bucket frequency (with shuffle=True)\")\n",
        "for b, c in sorted(probe.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
        "    print(f\"bucket {b:>12}: {c} batches\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "def to_checkerboard_grid(rgba: torch.Tensor, nrow: int = 4, tile: int = 16):\n",
        "    \"\"\"Compose RGBA onto a checkerboard and return a grid tensor in [0,1].\"\"\"\n",
        "    if rgba.dim() == 3:\n",
        "        rgba = rgba.unsqueeze(0)\n",
        "    assert rgba.shape[1] >= 4, \"Expected RGBA input\"\n",
        "    rgb = rgba[:, :3]\n",
        "    alpha = rgba[:, 3:4]\n",
        "\n",
        "    # If values look like [-1,1], map to [0,1]\n",
        "    if rgb.min() < -0.01 or rgb.max() > 1.01 or alpha.min() < -0.01 or alpha.max() > 1.01:\n",
        "        rgb = (rgb + 1.0) * 0.5\n",
        "        alpha = (alpha + 1.0) * 0.5\n",
        "\n",
        "    rgb = rgb.clamp(0.0, 1.0)\n",
        "    alpha = alpha.clamp(0.0, 1.0)\n",
        "\n",
        "    _, _, h, w = rgb.shape\n",
        "    y = torch.arange(h).view(-1, 1)\n",
        "    x = torch.arange(w).view(1, -1)\n",
        "    pattern = ((y // tile + x // tile) % 2).to(dtype=rgb.dtype)\n",
        "    pattern = pattern * 0.9 + 0.1\n",
        "    checker = pattern.unsqueeze(0).repeat(3, 1, 1)  # (3, H, W)\n",
        "    checker = checker.unsqueeze(0).repeat(rgb.shape[0], 1, 1, 1)\n",
        "\n",
        "    composed = rgb * alpha + checker * (1.0 - alpha)\n",
        "    return make_grid(composed, nrow=nrow, padding=2)\n",
        "\n",
        "\n",
        "def get_batch(loader, idx: int):\n",
        "    for i, batch in enumerate(loader):\n",
        "        if i == idx:\n",
        "            return batch\n",
        "    raise IndexError(f\"batch {idx} out of range\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Change this to step through batches\n",
        "batch_idx = 1  # e.g., 0, 1, 2...\n",
        "nrow = 4       # 4 x 4 grid for 16 examples\n",
        "\n",
        "batch = get_batch(train_loader, batch_idx)\n",
        "rgba = batch.get(\"composite\")\n",
        "if rgba is None:\n",
        "    rgba = batch.get(\"component\")\n",
        "if rgba is None:\n",
        "    raise ValueError(\"Batch missing 'composite' or 'component' tensor\")\n",
        "\n",
        "print(\"Batch shape:\", rgba.shape)\n",
        "print(\"Min/Max:\", rgba.min().item(), rgba.max().item())\n",
        "\n",
        "grid = to_checkerboard_grid(rgba, nrow=nrow)\n",
        "plt.figure(figsize=(12, 12))\n",
        "plt.imshow(grid.permute(1, 2, 0).cpu().numpy())\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total entries: 2831275\n",
            "per dataset counts: Counter({'lica_250k': 2651409, 'laion1M_aethtetic': 96113, 'prism_layer_pro': 80184, 'prism_layer_real': 3569})\n",
            "bucket count (top 10): [('w1024-h1024', 2058520), ('w960-h1152', 286172), ('w832-h1024', 274737), ('w768-h1408', 61603), ('w832-h1280', 35790), ('w1024-h832', 18288), ('w1408-h768', 9933), ('w1280-h832', 5546), ('w768-h512', 4582), ('w1216-h896', 4181)]\n",
            "bucket prob (top 10): [('w1024-h1024', 0.7270646616806916), ('w960-h1152', 0.10107531059328395), ('w832-h1024', 0.09703649415899197), ('w768-h1408', 0.02175804187159495), ('w832-h1280', 0.012640947982799269), ('w1024-h832', 0.006459280712753088), ('w1408-h768', 0.0035083133923762264), ('w1280-h832', 0.0019588348005757125), ('w768-h512', 0.0016183521558308536), ('w1216-h896', 0.0014767198523633345)]\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter, defaultdict\n",
        "from pathlib import Path\n",
        "import yaml\n",
        "from src.data_generation.bucket_dataset import build_bucket_entries, MixedBucketDataset\n",
        "\n",
        "cfg = yaml.safe_load(open(\"configs/flux_vae.yaml\"))\n",
        "data_cfg = cfg[\"data\"]\n",
        "split = data_cfg.get(\"bucket_split\", \"train\")\n",
        "ds_cfgs = data_cfg.get(\"bucket_datasets\", [])\n",
        "\n",
        "# 엔트리 생성\n",
        "entries = build_bucket_entries(ds_cfgs, split=split)\n",
        "print(\"total entries:\", len(entries))\n",
        "\n",
        "# 데이터셋별 크기 (root 폴더명 기준으로 구분)\n",
        "by_dataset = Counter(Path(e[\"root_dir\"]).name for e in entries)\n",
        "print(\"per dataset counts:\", by_dataset)\n",
        "\n",
        "# 버킷별 크기\n",
        "by_bucket = Counter(e[\"bucket\"] for e in entries)\n",
        "print(\"bucket count (top 10):\", by_bucket.most_common(10))\n",
        "\n",
        "# 버킷별 비율\n",
        "total = len(entries)\n",
        "bucket_probs = {b: c/total for b, c in by_bucket.items()}\n",
        "print(\"bucket prob (top 10):\", sorted(bucket_probs.items(), key=lambda x: x[1], reverse=True)[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# model load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of AutoencoderKL were not initialized from the model checkpoint at /home/ubuntu/ragb-vae/checkpoints/flux_rgba_vae_init and are newly initialized because the shapes did not match:\n",
            "- decoder.conv_out.bias: found shape torch.Size([4]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
            "- decoder.conv_out.weight: found shape torch.Size([4, 128, 3, 3]) in the checkpoint and torch.Size([3, 128, 3, 3]) in the model instantiated\n",
            "- encoder.conv_in.weight: found shape torch.Size([128, 4, 3, 3]) in the checkpoint and torch.Size([128, 3, 3, 3]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 6.50 GiB. GPU 0 has a total capacity of 79.19 GiB of which 1.45 GiB is free. Process 329632 has 63.33 GiB memory in use. Including non-PyTorch memory, this process has 14.36 GiB memory in use. Of the allocated memory 13.72 GiB is allocated by PyTorch, and 36.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     26\u001b[39m model.eval();\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     recon, _ = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrgba\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_dtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m recon = recon.cpu()\n\u001b[32m     32\u001b[39m recon_grid = to_checkerboard_grid(recon, nrow=nrow)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/jaepoong/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/jaepoong/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/ragb-vae/src/models/rgba_vae.py:277\u001b[39m, in \u001b[36mRgbaVAE.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    275\u001b[39m x_rgba = _ensure_alpha(x)\n\u001b[32m    276\u001b[39m vae_input = _to_vae_range(x_rgba)\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m posterior = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvae\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvae_input\u001b[49m\u001b[43m)\u001b[49m.latent_dist\n\u001b[32m    278\u001b[39m z = posterior.sample()\n\u001b[32m    279\u001b[39m recon = \u001b[38;5;28mself\u001b[39m.vae.decode(z).sample\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/jaepoong/lib/python3.11/site-packages/diffusers/utils/accelerate_utils.py:46\u001b[39m, in \u001b[36mapply_forward_hook.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_hf_hook\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m._hf_hook, \u001b[33m\"\u001b[39m\u001b[33mpre_forward\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     45\u001b[39m     \u001b[38;5;28mself\u001b[39m._hf_hook.pre_forward(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/jaepoong/lib/python3.11/site-packages/diffusers/models/autoencoders/autoencoder_kl.py:192\u001b[39m, in \u001b[36mAutoencoderKL.encode\u001b[39m\u001b[34m(self, x, return_dict)\u001b[39m\n\u001b[32m    190\u001b[39m     h = torch.cat(encoded_slices)\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m     h = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    194\u001b[39m posterior = DiagonalGaussianDistribution(h)\n\u001b[32m    196\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/jaepoong/lib/python3.11/site-packages/diffusers/models/autoencoders/autoencoder_kl.py:166\u001b[39m, in \u001b[36mAutoencoderKL._encode\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.use_tiling \u001b[38;5;129;01mand\u001b[39;00m (width > \u001b[38;5;28mself\u001b[39m.tile_sample_min_size \u001b[38;5;129;01mor\u001b[39;00m height > \u001b[38;5;28mself\u001b[39m.tile_sample_min_size):\n\u001b[32m    164\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tiled_encode(x)\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m enc = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.quant_conv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    168\u001b[39m     enc = \u001b[38;5;28mself\u001b[39m.quant_conv(enc)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/jaepoong/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/jaepoong/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/jaepoong/lib/python3.11/site-packages/diffusers/models/autoencoders/vae.py:168\u001b[39m, in \u001b[36mEncoder.forward\u001b[39m\u001b[34m(self, sample)\u001b[39m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# down\u001b[39;00m\n\u001b[32m    167\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m down_block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.down_blocks:\n\u001b[32m--> \u001b[39m\u001b[32m168\u001b[39m         sample = \u001b[43mdown_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    170\u001b[39m     \u001b[38;5;66;03m# middle\u001b[39;00m\n\u001b[32m    171\u001b[39m     sample = \u001b[38;5;28mself\u001b[39m.mid_block(sample)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/jaepoong/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/jaepoong/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/jaepoong/lib/python3.11/site-packages/diffusers/models/unets/unet_2d_blocks.py:1442\u001b[39m, in \u001b[36mDownEncoderBlock2D.forward\u001b[39m\u001b[34m(self, hidden_states, *args, **kwargs)\u001b[39m\n\u001b[32m   1439\u001b[39m     deprecate(\u001b[33m\"\u001b[39m\u001b[33mscale\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m1.0.0\u001b[39m\u001b[33m\"\u001b[39m, deprecation_message)\n\u001b[32m   1441\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m resnet \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.resnets:\n\u001b[32m-> \u001b[39m\u001b[32m1442\u001b[39m     hidden_states = \u001b[43mresnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemb\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   1444\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.downsamplers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1445\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m downsampler \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.downsamplers:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/jaepoong/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/jaepoong/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/jaepoong/lib/python3.11/site-packages/diffusers/models/resnet.py:328\u001b[39m, in \u001b[36mResnetBlock2D.forward\u001b[39m\u001b[34m(self, input_tensor, temb, *args, **kwargs)\u001b[39m\n\u001b[32m    325\u001b[39m hidden_states = input_tensor\n\u001b[32m    327\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.norm1(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m328\u001b[39m hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnonlinearity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.upsample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    331\u001b[39m     \u001b[38;5;66;03m# upsample_nearest_nhwc fails with large batch sizes. see https://github.com/huggingface/diffusers/issues/984\u001b[39;00m\n\u001b[32m    332\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m hidden_states.shape[\u001b[32m0\u001b[39m] >= \u001b[32m64\u001b[39m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/jaepoong/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/jaepoong/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/jaepoong/lib/python3.11/site-packages/torch/nn/modules/activation.py:473\u001b[39m, in \u001b[36mSiLU.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    469\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m    470\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    471\u001b[39m \u001b[33;03m    Runs the forward pass.\u001b[39;00m\n\u001b[32m    472\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m473\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43msilu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/jaepoong/lib/python3.11/site-packages/torch/nn/functional.py:2371\u001b[39m, in \u001b[36msilu\u001b[39m\u001b[34m(input, inplace)\u001b[39m\n\u001b[32m   2369\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[32m   2370\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch._C._nn.silu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2371\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_nn\u001b[49m\u001b[43m.\u001b[49m\u001b[43msilu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 6.50 GiB. GPU 0 has a total capacity of 79.19 GiB of which 1.45 GiB is free. Process 329632 has 63.33 GiB memory in use. Including non-PyTorch memory, this process has 14.36 GiB memory in use. Of the allocated memory 13.72 GiB is allocated by PyTorch, and 36.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ],
      "source": [
        "\n",
        "from src.models import RgbaVAE, composite_over_black\n",
        "\n",
        "# Resolve device and subfolder for flux/qwen variants\n",
        "_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "_model_cfg = cfg.get(\"model\", {})\n",
        "_base_arch = str(_model_cfg.get(\"base_arch\", \"qwen\")).lower()\n",
        "_default_subfolder = \"ae\" if \"flux\" in _base_arch else \"vae\"\n",
        "_subfolder = _model_cfg.get(\"rgb_subfolder\")\n",
        "_subfolder = _default_subfolder if _subfolder is None else _subfolder\n",
        "_ckpt_dir = _model_cfg.get(\"rgb_checkpoint\", \"checkpoints/flux_rgba_vae_init\")\n",
        "\n",
        "_dtype =  torch.float32\n",
        "model = RgbaVAE.from_pretrained_rgb(\n",
        "    model_name_or_path=_ckpt_dir,\n",
        "    subfolder=_subfolder,\n",
        "    torch_dtype=_dtype,\n",
        "    alpha_bias_init=_model_cfg.get(\"alpha_bias_init\", 0.0),\n",
        "    beta=_model_cfg.get(\"beta\", 0.25),\n",
        "    alpha_loss_weight=_model_cfg.get(\"alpha_loss_weight\", 1.0),\n",
        "    alpha_l1_weight=_model_cfg.get(\"alpha_l1_weight\", 0.0),\n",
        "    rgb_loss_weight=_model_cfg.get(\"rgb_loss_weight\", 1.0),\n",
        "    white_bg_weight=_model_cfg.get(\"white_bg_loss_weight\", 0.0),\n",
        "    black_bg_weight=_model_cfg.get(\"black_bg_loss_weight\", 0.0),\n",
        "    device=_device,\n",
        ")\n",
        "model.eval();\n",
        "\n",
        "with torch.no_grad():\n",
        "    recon, _ = model(rgba.to(device=_device, dtype=_dtype))\n",
        "recon = recon.cpu()\n",
        "\n",
        "recon_grid = to_checkerboard_grid(recon, nrow=nrow)\n",
        "plt.figure(figsize=(12, 12))\n",
        "plt.imshow(recon_grid.permute(1, 2, 0).numpy())\n",
        "plt.title(\"Reconstruction over checkerboard\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "composited = composite_over_black(recon)\n",
        "comp_grid = make_grid(composited, nrow=nrow, padding=2)\n",
        "plt.figure(figsize=(12, 12))\n",
        "plt.imshow(comp_grid.permute(1, 2, 0).numpy())\n",
        "plt.title(\"Reconstruction over black\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8832ecf8beb74d43a6a39be5271055dc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "dataloader-dryrun:   0%|          | 0/177025 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch 50: shapes -> comp (16, 4, 960, 640)\n",
            "batch 100: shapes -> comp (16, 4, 1024, 1024)\n",
            "batch 150: shapes -> comp (16, 4, 1024, 1024)\n",
            "batch 200: shapes -> comp (16, 4, 1024, 832)\n",
            "batch 250: shapes -> comp (16, 4, 1024, 1024)\n",
            "batch 300: shapes -> comp (16, 4, 1024, 1024)\n",
            "batch 350: shapes -> comp (16, 4, 1024, 1024)\n",
            "batch 400: shapes -> comp (16, 4, 1024, 1024)\n",
            "batch 450: shapes -> comp (16, 4, 1024, 1024)\n",
            "batch 500: shapes -> comp (16, 4, 1024, 832)\n",
            "batch 550: shapes -> comp (16, 4, 1024, 1024)\n",
            "batch 600: shapes -> comp (16, 4, 1024, 1024)\n",
            "batch 650: shapes -> comp (16, 4, 1024, 1024)\n",
            "batch 700: shapes -> comp (16, 4, 1152, 960)\n",
            "batch 750: shapes -> comp (16, 4, 1152, 960)\n",
            "batch 800: shapes -> comp (16, 4, 1024, 1024)\n",
            "batch 850: shapes -> comp (16, 4, 1024, 1024)\n",
            "batch 900: shapes -> comp (16, 4, 1024, 1024)\n",
            "batch 950: shapes -> comp (16, 4, 832, 1280)\n",
            "batch 1000: shapes -> comp (16, 4, 1024, 1024)\n",
            "batch 1050: shapes -> comp (16, 4, 1024, 1024)\n",
            "batch 1100: shapes -> comp (16, 4, 1024, 1024)\n",
            "batch 1150: shapes -> comp (16, 4, 1024, 1024)\n",
            "batch 1200: shapes -> comp (16, 4, 1408, 768)\n",
            "batch 1250: shapes -> comp (16, 4, 1024, 1024)\n",
            "batch 1300: shapes -> comp (16, 4, 1024, 1024)\n",
            "batch 1350: shapes -> comp (16, 4, 1024, 1024)\n",
            "batch 1400: shapes -> comp (16, 4, 1024, 1024)\n",
            "batch 1450: shapes -> comp (16, 4, 1024, 1024)\n",
            "batch 1500: shapes -> comp (16, 4, 1024, 1024)\n",
            "batch 1550: shapes -> comp (16, 4, 1152, 960)\n",
            "batch 1600: shapes -> comp (16, 4, 1024, 1024)\n",
            "batch 1650: shapes -> comp (16, 4, 1024, 1024)\n",
            "batch 1700: shapes -> comp (16, 4, 1024, 1024)\n",
            "batch 1750: shapes -> comp (16, 4, 1024, 1024)\n",
            "batch 1800: shapes -> comp (16, 4, 1024, 1024)\n",
            "batch 1850: shapes -> comp (16, 4, 1024, 1024)\n",
            "batch 1900: shapes -> comp (16, 4, 1024, 832)\n",
            "batch 1950: shapes -> comp (16, 4, 1024, 832)\n",
            "batch 2000: shapes -> comp (16, 4, 1024, 1024)\n",
            "batch 2050: shapes -> comp (16, 4, 1024, 1024)\n",
            "batch 2100: shapes -> comp (16, 4, 1024, 1024)\n",
            "batch 2150: shapes -> comp (16, 4, 1024, 832)\n",
            "batch 2200: shapes -> comp (16, 4, 1024, 1024)\n",
            "batch 2250: shapes -> comp (16, 4, 1024, 1024)\n",
            "batch 2300: shapes -> comp (16, 4, 1024, 832)\n",
            "batch 2350: shapes -> comp (16, 4, 1024, 1024)\n",
            "batch 2400: shapes -> comp (16, 4, 1024, 832)\n",
            "batch 2450: shapes -> comp (16, 4, 1024, 1024)\n",
            "batch 2500: shapes -> comp (16, 4, 1024, 1024)\n",
            "batch 2550: shapes -> comp (16, 4, 1024, 1024)\n",
            "batch 2600: shapes -> comp (16, 4, 1024, 832)\n",
            "batch 2650: shapes -> comp (16, 4, 1024, 1024)\n",
            "batch 2700: shapes -> comp (16, 4, 1024, 1024)\n",
            "batch 2750: shapes -> comp (16, 4, 1024, 1024)\n",
            "batch 2800: shapes -> comp (16, 4, 1024, 1024)\n",
            "batch 2850: shapes -> comp (16, 4, 1024, 1024)\n",
            "batch 2900: shapes -> comp (16, 4, 1024, 1024)\n",
            "batch 2950: shapes -> comp (16, 4, 1024, 1024)\n",
            "batch 3000: shapes -> comp (16, 4, 1024, 1024)\n",
            "batch 3050: shapes -> comp (16, 4, 1024, 1024)\n",
            "batch 3100: shapes -> comp (16, 4, 1024, 1024)\n",
            "batch 3150: shapes -> comp (16, 4, 1024, 1024)\n",
            "batch 3200: shapes -> comp (16, 4, 832, 1024)\n",
            "batch 3250: shapes -> comp (16, 4, 1152, 960)\n",
            "batch 3300: shapes -> comp (16, 4, 768, 1408)\n",
            "batch 3350: shapes -> comp (16, 4, 1024, 1024)\n",
            "batch 3400: shapes -> comp (16, 4, 1024, 1024)\n",
            "batch 3450: shapes -> comp (16, 4, 1024, 1024)\n",
            "batch 3500: shapes -> comp (16, 4, 1024, 1024)\n",
            "batch 3550: shapes -> comp (16, 4, 1024, 1024)\n",
            "batch 3600: shapes -> comp (16, 4, 1024, 1024)\n",
            "batch 3650: shapes -> comp (16, 4, 1024, 1024)\n",
            "batch 3700: shapes -> comp (16, 4, 1152, 960)\n",
            "batch 3750: shapes -> comp (16, 4, 1024, 1024)\n",
            "batch 3800: shapes -> comp (16, 4, 1024, 1024)\n",
            "batch 3850: shapes -> comp (16, 4, 1024, 1024)\n",
            "batch 3900: shapes -> comp (16, 4, 1024, 832)\n",
            "batch 3950: shapes -> comp (16, 4, 1024, 832)\n",
            "batch 4000: shapes -> comp (16, 4, 1152, 960)\n",
            "batch 4050: shapes -> comp (16, 4, 1024, 1024)\n",
            "batch 4100: shapes -> comp (16, 4, 1408, 768)\n",
            "batch 4150: shapes -> comp (16, 4, 1024, 1024)\n",
            "batch 4200: shapes -> comp (16, 4, 1024, 1024)\n",
            "batch 4250: shapes -> comp (16, 4, 1024, 832)\n",
            "batch 4300: shapes -> comp (16, 4, 576, 768)\n",
            "batch 4350: shapes -> comp (16, 4, 1024, 1024)\n",
            "batch 4400: shapes -> comp (16, 4, 1024, 1024)\n",
            "batch 4450: shapes -> comp (16, 4, 1024, 832)\n",
            "batch 4500: shapes -> comp (16, 4, 1024, 1024)\n",
            "batch 4550: shapes -> comp (16, 4, 1024, 1024)\n",
            "batch 4600: shapes -> comp (16, 4, 1024, 1024)\n",
            "batch 4650: shapes -> comp (16, 4, 1024, 832)\n",
            "batch 4700: shapes -> comp (16, 4, 1024, 1024)\n",
            "batch 4750: shapes -> comp (16, 4, 1024, 832)\n",
            "batch 4800: shapes -> comp (16, 4, 1024, 1024)\n",
            "batch 4850: shapes -> comp (16, 4, 1024, 1024)\n",
            "batch 4900: shapes -> comp (16, 4, 1024, 1024)\n",
            "batch 4950: shapes -> comp (16, 4, 1024, 1024)\n",
            "batch 5000: shapes -> comp (16, 4, 1152, 960)\n",
            "batch 5050: shapes -> comp (16, 4, 1024, 1024)\n",
            "batch 5100: shapes -> comp (16, 4, 1024, 1024)\n",
            "batch 5150: shapes -> comp (16, 4, 1024, 1024)\n",
            "batch 5200: shapes -> comp (16, 4, 1024, 1024)\n",
            "batch 5250: shapes -> comp (16, 4, 1024, 832)\n",
            "batch 5300: shapes -> comp (16, 4, 1024, 1024)\n",
            "batch 5350: shapes -> comp (16, 4, 1024, 1024)\n",
            "batch 5400: shapes -> comp (16, 4, 1152, 960)\n",
            "batch 5450: shapes -> comp (16, 4, 1024, 1024)\n",
            "batch 5500: shapes -> comp (16, 4, 1152, 960)\n",
            "batch 5550: shapes -> comp (16, 4, 1024, 1024)\n",
            "batch 5600: shapes -> comp (16, 4, 1024, 1024)\n",
            "batch 5650: shapes -> comp (16, 4, 1280, 832)\n",
            "batch 5700: shapes -> comp (16, 4, 1024, 1024)\n",
            "batch 5750: shapes -> comp (16, 4, 832, 768)\n",
            "batch 5800: shapes -> comp (16, 4, 1152, 960)\n",
            "batch 5850: shapes -> comp (16, 4, 1024, 1024)\n",
            "batch 5900: shapes -> comp (16, 4, 1024, 1024)\n",
            "batch 5950: shapes -> comp (16, 4, 1152, 960)\n",
            "batch 6000: shapes -> comp (16, 4, 1024, 1024)\n",
            "batch 6050: shapes -> comp (16, 4, 1024, 1024)\n"
          ]
        }
      ],
      "source": [
        "import traceback\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Dry-run the train dataloader to catch corrupt files quickly.\n",
        "# Set max_batches=None to scan the whole epoch (may take a long time).\n",
        "max_batches = None  # adjust as needed\n",
        "\n",
        "try:\n",
        "    total_hint = len(train_loader)\n",
        "except TypeError:\n",
        "    total_hint = None\n",
        "progress_total = max_batches if max_batches is not None else total_hint\n",
        "\n",
        "completed = 0\n",
        "failed_step = None\n",
        "step = -1\n",
        "\n",
        "try:\n",
        "    for step, batch in tqdm(\n",
        "        enumerate(train_loader),\n",
        "        total=progress_total,\n",
        "        desc=\"dataloader-dryrun\",\n",
        "    ):\n",
        "        if max_batches is not None and step >= max_batches:\n",
        "            break\n",
        "\n",
        "        # Touch tensors to force materialization\n",
        "        composite = batch.get(\"composite\")\n",
        "        component = batch.get(\"component\")\n",
        "        background = batch.get(\"background\")\n",
        "        _ = composite.shape if composite is not None else None\n",
        "        _ = component.shape if component is not None else None\n",
        "        _ = background.shape if background is not None else None\n",
        "\n",
        "        if (step + 1) % 50 == 0:\n",
        "            print(f\"batch {step+1}: shapes -> comp {None if composite is None else tuple(composite.shape)}\")\n",
        "        completed = step + 1\n",
        "except Exception as exc:  # surface the first failure\n",
        "    failed_step = step\n",
        "    print(f\"[ERROR] failed at batch {step}: {exc}\")\n",
        "    traceback.print_exc()\n",
        "finally:\n",
        "    print(f\"completed batches: {completed} / {max_batches or total_hint or 'unknown'}\")\n",
        "    if failed_step is not None:\n",
        "        print(f\"first failure was at batch {failed_step}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "jaepoong",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
